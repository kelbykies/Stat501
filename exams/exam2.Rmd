---
title: "Stat501_Exam2"
author: "Kelby Kies"
date: "4/17/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1: 

## Part 1. Our objective is to reduce the dimensions of the data to summarize the variation in the dataset.

### Part 1A.): Is it appropriate to scale the dataset before performing aprincipal components analysis? [5points]


```{r}
# Read in the places.txt file
places <- read.table('~/Desktop/stat_501/places.txt', header = T)
# Put data in log10 scale
places <- data.frame(log10(places[,1:9]), rank = places[,10])

# Is appropriate to scale the dataset before PCA?
apply(X = places[, -10], MARGIN = 2, FUN = sd)
# Yes!
```
We should scale the dataset to do a principal component analysis because the largest sd for 'The Arts' = 0.54513590 and it is about 10 times larger than the lowest sd for 'Education' = 0.05019937. When there is such a difference in sd between variables, it is appropriate to scale. 


### Part 1B.): Perform a principal components analysis using the correlation matrix. What is the minimum number of components needed to display at least 80% of the variation in the dataset? [10 points]
```{r}
# Perform PCA
place_pca <- prcomp(places[,1:9], scale = T)
summary(place_pca)$importance
```
The number of principal components that are used to display at least 80% of the total variation in the dataset is 5. With 5 principal components at least ~82% of the total variation is accounted for. 

### Part 1C.): Discuss the composition of the first three principal components.[10 points]
```{r}
pairs(place_pca$x[,1:3],
      panel=function(x,y){panel.smooth(x,y, col = as.numeric(as.factor(colnames(places[1:9]))),
        pch = 20, cex = 1)
        abline(lsfit(x,y),lty=2) })
#legend("bottomright", fill = as.factor(colnames(places[1:9])), legend = c( levels(as.factor(colnames(places[1:9])))))



```
I wasn't able to figure out how to add a legend to my plot, but here the points are colored by the different 9 variables. Here we can se that there isn't any real separation between the 9 variables as we might have expected. The PC1/PC2 plot seems to have more variance than the other plots which makes sense since we would expect the first PCs to contain the majority of the variance. 
The first component accounts for 36.6% of the total variation. The second principal component accounts for ~13.5% of the total variance and the third principal component accounts fro ~12.3% of the total variance. 


## Part 2: We will now do a factor analysis of the dataset.

### Part 2A.) Perform a factor analysis of the dataset, using BIC to estimate the optimal number of factors needed to display the data. Use the quartimax rotation. [10 points]
```{r}
library(fad)
places_fad <- fad(x=as.matrix(places[,-10]), factors=1, scores = "regression",rotation = "quartimax", method = "mle")
bic <- NULL
for (q in 0:5)
  {
  res <- fad(x=as.matrix(places[,-10]), factors=q, scores = "regression",rotation = "quartimax", method = "mle")
  bic <- c(bic, res$BIC)
}
# What is the lowest BIC value? THis corresponds to the optimal # of factos needed to display the data. 
which.min(bic)-1
min(bic)
# 2 factors!
```
I ran a factor analysis for q values from 0 to 5 on the places data. The minimum BIC value, -8805.389, value was generated during the factor analysis with 2 factors and also using the quartimax rotation. This indicates that 2 factors are optimum to display the data. 

### Part 2B.) Interpret the first two factors. [5 points]
```{r}
places_fda <- fad(x=as.matrix(places[,-10]), factors=2, scores = "regression",rotation = "quartimax", method = "mle")
places_fda$loadings
```
The first factor is a weighted mean of the variables Climate/Terrain, Housing, Health Care/Environment, Crime, Transportation, Education, Arts, Recreation and Economics. The second factor shows a contrast between the (Health Care/Environment & Education) variables and the (Climate/Terrain, Houseing, Crime, Transportation, Arts, Recreation and Economics) variables. 

Loadings close to -1 or 1 indicate that the factor strongly influences the variable. Loadings close to 0 indicate that the factor has a weak influence on the variable. We can see the first factor has the most influence on Health Care/Environment and Arts, while the second factor has the strongest influence on Recreation. 

The 2 factors explain about 38.4% of the total variance. 

### Part 2C.) What is the difference between factor analysis and principal components analsis? [5 points]

PCA is a linear combination of variables; Factor Analysis is a measurement model of a latent variable.

The two analyses are very similar, but differ slightly. A PCA is used to extra linear composites of observed variables. A Factor analysis explicitly assumes existence of a latent variable that underlies the observed data.
The var-Cov matrix for the two analyses also differs.
For PCA:
$$ \Sigma_{pxp} \cong \Gamma_{pxq} \Lambda \Gamma_{qxp}'$$
But for FDA: 
$$ \Sigma_{pxp} =  \Lambda_{pxq} \Lambda_{qxp}' + \Delta $$ 
Where $$ \Delta $$ is a diagonal matrix. 

For FDA the Lambda matrix does not have to be orthogonal and Sigma is equal to this value rather than just an approximation. 

# Question 2

## Part 1:Perform a multivariate mutiple regression analysis to understand which of the four pulp characteristics have linear relationships with the properties of the paper data. [10 points]

```{r}
library(car)
# Read in the pulp_paper data
pulp_paper <- read.table('~/Desktop/stat_501/pulp_paper.dat', header=T)
pulp_paper_model <- lm(cbind(y1, y2, y3, y4) ~ z1 + z2 + z3 + z4, data = pulp_paper)
pulp_paper_model
pulp_manova <- Manova(pulp_paper_model)
summary(pulp_manova)
```

## Part 2:Estimate the matrix of coefficients. [10 points]

```{r}
new_model <- lm(cbind(y1, y2, y3, y4) ~ z2 + z3 + z4, data = pulp_paper)
new_model$coefficients
#pulp_paper_model$coefficients
```
## Part 3: Provide an estimate of the variance-covariance matrix of the error in the four response variables after accounting for the linear effect of the four fiber characteristics. [10 points]
```{r}
cov(new_model$residuals)
#cov(pulp_paper_model$residuals)
```
## Part 4: Provide a detailed analysis and interpretation of your results.[10 points]
I created a multiple regression model that had paper characteristic (y1 - y4) variables as the response variables and pulp characteristics (z1 - z4) as the predictor variables. I ran a Manova to see what pulp characteristics had a significant effect on the response variables. What I found is that long fiber fraction (z2), fine fiber fraction (z3) and zero span tensile (z4) had significant effect on the paper characteristics while arithmetic fiber length did not. Long fiber fraction (z2) and zero span tensile(z4) had the most significant effects on the paper characterisitics at the 0.0001 significance level with p-values of 1.009e-05 and 2.308e-13, respectively. The fine fiber fraction (z3) had significant effect on the paper characteristics at the 0.05 with a p-value of 0.01100.

Because Arithmetic fiber length did not have a significant effect, it was removed from the model. 

For Part 2.) I have provided the matrix of coefficient values after removing the z1 variable from the model.It looks like Zero span tensile has the largest effect on the paper characteristics because if we held everything else constant and increased z4, then y1 increases by 82.53, y2 increases by 27.78, y3 increases by 44.58 and y4 will increase by 15.77. In order words zero span tensile has the largest coefficient for all of the variables. 

For Part 3.) I have shown the variance-covariance matrix of the error (or residuals) for the 4 response variables after removing the arithmetic fiber length from the model. 
We can see that the errors associated with y3 and y1 have the strongest correlation of 0.9004985 while the errors associated with y4 and y2 have the weakest correlation, 0.09000275. Errors associated with y1 also have a very large varince of 2.1945266 .

## Part 5: Provide a canonical correlation analysis of the properties of paper (y1, y2, y3, y4) and the pulp fiber characteristics (z1, z2, z3, z4). Discuss the results. [15 points]
```{r}
paper <- pulp_paper[,1:4]
pulp <- pulp_paper[,5:8]
cancor(paper, pulp)
```
The canonical correlation analysis shows that there is a very strong, positive linear relationship between the paper characteristics and pulp characterisitcs because the first canonical correlation is 0.91732930, which is close to 1. After this is removed in the first canonical correlation,the 2nd canonical correlation continues to show a strong linear relationship with a value of 0.81692694. After this is removed, their is very little further linear relationship. 
